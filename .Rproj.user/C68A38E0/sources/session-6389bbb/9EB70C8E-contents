---
title: "Lancet Paper supplementary material"
output: 
  bookdown::html_document2:
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: no
    theme: flatly
    highlight: kate 
    fig.caption: yes 
    tables: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  fig.width = 12,
  fig.height = 8
)
```

```{css, echo = F}
<!-- .header1 { -->
<!-- color: #8a2587; -->
<!-- } -->

.caption {
margin: auto;
text-align: left;
}
```

```{r, warning = F, message = F, include = F, echo = F}
myColour <- "#8a2587"
```

# Introduction

The supplementary material for the lancet paper looking at how different service components effect key clinical outcomes and health care use. This additional material demonstrates on a set of synthetic data, the data processing and analysis carried out for the paper "X". 

# Library 

These are the packages that were used in the analysis of the data. Please note, the version numbers might be different, but the methods are still the same. 

```{r load libraries, warning = F, message = F}
# packages for analysis and processing
library(tidyverse)
library(tidybayes)
library(brms)
library(lubridate)

# packages for figures and tables in this document 
library(see)
library(ggridges)
```

```{r figure settings, echo = F}
# Sort out the theme
textSize <- 17:20
theme_set(theme_bw())
theme_update(
  title = element_text(size = textSize[3]),
  axis.text = element_text(size = textSize[2]),
  axis.title = element_text(size = textSize[3]),
  legend.text = element_text(size = textSize[1])
)
```

# Load data 

This loads in a dataset for the posterior samples from one of the models fit in the paper "X". This is from the multivariable analysis regarding the rates of Serious Infection. The data is arranged in a long format with the `iter` column linking draws from the posterior across the different parameters. Due to the link functions used in the `brms` package when fitting a negative binomial model, all coefficients for the mu value are on a log scale. For simplicity, coefficients for socio demographic features are not included in this dataset. 

```{r load data, message = F}
df_SI <- read_csv("data/df_SI.csv")

head(df_SI)
```

There are a few rows that aren't needed for this demonstration, so the dataset will tidied up to make things easier to generate a synthetic dataset. 

```{r clean up data}
df_SI <- df_SI %>% 
  select(iter, param, Coef) %>% 
  spread(param, Coef) %>% 
  select(iter, shape, Intercept, everything())

head(df_SI)
```

Now the posterior draws are in an easier format to use to make some synthetic data. As reported in the paper, different health boards had access to different service components. To this end, I will use the survey responses to generate a set of subjects for each synthetic board who has access to the respective components. 

```{r generate subject data, message = F}
df_survResps <- read_csv("data/surveyResponses.csv") %>% 
  # will help make things a little less confusing if I use a more descriptive label
  rename(Board = ID) %>% 
  # need to tidy up the columns so they match the ones in the df_SI dataset
  mutate(
    # combine two responses
    AdviceandorLed = as.numeric(NurseAdviceLine | NurseLedClinic), 
    # invert General Clinic value for Cohorted Clinics
    CohortedClinic = abs(GeneralClinic - 1),
    # Make columns for intercept and shape that are always on 
    Intercept = 1,
    shape = 1
  )

df_survResps <- df_survResps %>% 
  select(Board, colnames(df_SI %>% select(-iter)))

df_survResps
```

Create a dataset with 100 patients per service and add a column with a random iter value ranging from 1 to `r nrow(df_SI)` to retrieve a random draw from the posterior. *There might be a better way to do this, but I think this should reflect the distribution itself?* Then I can combine the posterior draws dataset in order to get a value for the parameters in a negative binomial distribution. From this, I can then generate some synthetic data. 

```{r Sort out parameter values for various patients}
# create some patient data
df_patientData <- df_survResps %>% 
  expand_grid(subj = 1:100) %>% 
  mutate(iter = sample(1:nrow(df_SI), nrow(.), replace = T))

# create a dataset that has the parameter values for each row in the df_patientData
df_params <- tibble(iter = df_patientData$iter) %>% 
  left_join(df_SI)

# get the parameter values for each patient
for(i in 2:ncol(df_params)){
  df_patientData[,i] <- df_params[,i] * df_patientData[,i]
}  

# create the mu parameter needed for the negative binomial dataset
df_patientData <- df_patientData %>% 
  mutate(mu = exp(rowSums(across(c(Intercept:WaitTimeNewWithinWeek)))),
         # add in a random value for the time in study
         timeYears = runif(nrow(.), .1, 24))

head(df_patientData)
```

Now all the values are in the dataset, we can produce count data for the number of visits each patient had during the time they were in the study. 

```{r functions, include = F, echo = F}
# function to generate times for visits for positive events 
# This is the poisson process, so we use sample time to event from the exponential distribution
poisProc <- function(Rate, timeObs){
  count <- 0
  timeSum <- 0
  outTimes <- c()
  running <- T
  while(running){
    thisTime <- rexp(1, Rate)
    timeSum <- timeSum + thisTime
    if(timeSum > timeObs){
      running <- F
    } else {
      count <- count + 1
      outTimes <- c(outTimes, thisTime)
    }
  }
  return(outTimes)
}

# This will generate negative binomial distributed numbers for the total number of events 
# the process is the same as for the poisson data, but the lambda parameter is sampled 
# from a gamma distribution 
nbinomProcess <- function(mu, size, time){
  # get prob
  prob <- size / (size + mu)
  
  lambda <- rgamma(1, shape = size, rate = prob/(1 - prob))
  
  return(poisProc(lambda, time))
}

# TODO: 
# Need a function to make it so that every subject has at least one visits in the data 
# otherwise, out data set will look a bit wonky 

# TODO: 
# function to add in some ICD-10 looking codes just so we can screen for them. 

```




